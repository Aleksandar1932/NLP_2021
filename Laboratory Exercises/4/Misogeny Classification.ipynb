{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.5/bin\")\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.test import is_gpu_available\n",
    "\n",
    "from scripts.utils import load_data\n",
    "from scripts.model import recall_m, precision_m, f1_m, get_model_and_data\n",
    "\n",
    "WANDB_PROJECT_NAME = os.getenv(\"WANDB_PROJECT_NAME\") or \"[NLP] lab-04 | misogyny classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>misogynous</th>\n",
       "      <th>shaming</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>objectification</th>\n",
       "      <th>violence</th>\n",
       "      <th>Text Transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>not now, dad. We should burn Jon Snow. stop it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>there may have been a mixcommunication with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i shouldn't have sold my boat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bitches be like, It was my fault i made him mad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>find a picture of 4 girls together on FB make ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  misogynous  shaming  stereotype  objectification  violence  \\\n",
       "0    28.jpg           0        0           0                0         0   \n",
       "1    30.jpg           0        0           0                0         0   \n",
       "2    33.jpg           0        0           0                0         0   \n",
       "3    58.jpg           1        0           0                0         1   \n",
       "4    89.jpg           0        0           0                0         0   \n",
       "\n",
       "                                  Text Transcription  \n",
       "0  not now, dad. We should burn Jon Snow. stop it...  \n",
       "1  there may have been a mixcommunication with th...  \n",
       "2                      i shouldn't have sold my boat  \n",
       "3    Bitches be like, It was my fault i made him mad  \n",
       "4  find a picture of 4 girls together on FB make ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misogynous</th>\n",
       "      <th>shaming</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>objectification</th>\n",
       "      <th>violence</th>\n",
       "      <th>Text Transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>not now, dad. We should burn Jon Snow. stop it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>there may have been a mixcommunication with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i shouldn't have sold my boat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bitches be like, It was my fault i made him mad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>find a picture of 4 girls together on FB make ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   misogynous  shaming  stereotype  objectification  violence  \\\n",
       "0           0        0           0                0         0   \n",
       "1           0        0           0                0         0   \n",
       "2           0        0           0                0         0   \n",
       "3           1        0           0                0         1   \n",
       "4           0        0           0                0         0   \n",
       "\n",
       "                                  Text Transcription  \n",
       "0  not now, dad. We should burn Jon Snow. stop it...  \n",
       "1  there may have been a mixcommunication with th...  \n",
       "2                      i shouldn't have sold my boat  \n",
       "3    Bitches be like, It was my fault i made him mad  \n",
       "4  find a picture of 4 girls together on FB make ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=[\"file_name\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Text Transcription']\n",
    "y_task1 = df['misogynous']\n",
    "y_task2 = df[[\"shaming\", \"stereotype\", \"objectification\", \"violence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train_task1, y_test_task1 = train_test_split(X, y_task1, test_size=0.2, random_state=42)\n",
    "_, _, y_train_task2, y_test_task2 = train_test_split(X, y_task2, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbCallback\n",
    "from transformers import TFBertForSequenceClassification, TFAlbertForSequenceClassification, TFRobertaForSequenceClassification, TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and Biases is going to be used for logging model training and hyperparameter tuning. The project is available at [[NLP] lab-04 | misogyny classification](https://wandb.ai/aleksandar1932/[NLP]%20lab-04%20%7C%20misogyny%20classification?workspace=user-aleksandar1932)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb import wandb\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy, hinge\n",
    "\n",
    "run = wandb.init(project=WANDB_PROJECT_NAME, job_type=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TFBertForSequenceClassification-bert-base-cased with 2 labels\n",
      "Tokenizing data with BertTokenizerFast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 3722.08it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 3967.18it/s]\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, train_input_ids, train_attention_masks, test_input_ids, test_attention_masks = get_model_and_data(TFBertForSequenceClassification, 2, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                                  loss=hinge,\n",
    "                                  metrics=['accuracy', f1_m,precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ALEKSA~1\\AppData\\Local\\Temp/ipykernel_3388/3944956283.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Epoch 1/20\n",
      "8/8 - 17s - loss: 1.9672 - accuracy: 0.4359 - f1_m: 0.2755 - precision_m: 0.2208 - recall_m: 0.5500 - 17s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "8/8 - 2s - loss: 1.3417 - accuracy: 0.5128 - f1_m: 0.3250 - precision_m: 0.3145 - recall_m: 0.6562 - 2s/epoch - 250ms/step\n",
      "Epoch 3/20\n",
      "8/8 - 2s - loss: 1.1743 - accuracy: 0.4744 - f1_m: 0.5598 - precision_m: 0.4563 - recall_m: 0.8250 - 2s/epoch - 250ms/step\n",
      "Epoch 4/20\n",
      "8/8 - 2s - loss: 1.7785 - accuracy: 0.4231 - f1_m: 0.4595 - precision_m: 0.3406 - recall_m: 0.8125 - 2s/epoch - 253ms/step\n",
      "Epoch 5/20\n",
      "8/8 - 2s - loss: 1.8016 - accuracy: 0.5256 - f1_m: 0.2681 - precision_m: 0.2125 - recall_m: 0.3750 - 2s/epoch - 258ms/step\n",
      "Epoch 6/20\n",
      "8/8 - 2s - loss: 1.4309 - accuracy: 0.5385 - f1_m: 0.3009 - precision_m: 0.2401 - recall_m: 0.6458 - 2s/epoch - 257ms/step\n",
      "Epoch 7/20\n",
      "8/8 - 2s - loss: 0.9910 - accuracy: 0.4872 - f1_m: 0.4542 - precision_m: 0.3375 - recall_m: 0.7188 - 2s/epoch - 252ms/step\n",
      "Epoch 8/20\n",
      "8/8 - 2s - loss: 1.1438 - accuracy: 0.5256 - f1_m: 0.4724 - precision_m: 0.3746 - recall_m: 0.8432 - 2s/epoch - 253ms/step\n",
      "Epoch 9/20\n",
      "8/8 - 2s - loss: 1.1247 - accuracy: 0.5128 - f1_m: 0.2591 - precision_m: 0.1823 - recall_m: 0.5083 - 2s/epoch - 244ms/step\n",
      "Epoch 10/20\n",
      "8/8 - 2s - loss: 1.3823 - accuracy: 0.5128 - f1_m: 0.2671 - precision_m: 0.1708 - recall_m: 0.6875 - 2s/epoch - 256ms/step\n",
      "Epoch 11/20\n",
      "8/8 - 2s - loss: 1.0770 - accuracy: 0.5513 - f1_m: 0.3321 - precision_m: 0.2170 - recall_m: 0.7188 - 2s/epoch - 247ms/step\n",
      "Epoch 12/20\n",
      "8/8 - 2s - loss: 0.9942 - accuracy: 0.6026 - f1_m: 0.2411 - precision_m: 0.3267 - recall_m: 0.2804 - 2s/epoch - 241ms/step\n",
      "Epoch 13/20\n",
      "8/8 - 2s - loss: 1.1108 - accuracy: 0.5000 - f1_m: 0.3631 - precision_m: 0.3094 - recall_m: 0.5500 - 2s/epoch - 227ms/step\n",
      "Epoch 14/20\n",
      "8/8 - 2s - loss: 1.6539 - accuracy: 0.3462 - f1_m: 0.3161 - precision_m: 0.2250 - recall_m: 0.8601 - 2s/epoch - 226ms/step\n",
      "Epoch 15/20\n",
      "8/8 - 2s - loss: 1.9931 - accuracy: 0.5641 - f1_m: 0.3826 - precision_m: 0.2884 - recall_m: 0.9542 - 2s/epoch - 232ms/step\n",
      "Epoch 16/20\n",
      "8/8 - 2s - loss: 1.1351 - accuracy: 0.3846 - f1_m: 0.0455 - precision_m: 0.0278 - recall_m: 0.1250 - 2s/epoch - 235ms/step\n",
      "Epoch 17/20\n",
      "8/8 - 2s - loss: 1.0385 - accuracy: 0.6154 - f1_m: 0.2944 - precision_m: 0.2445 - recall_m: 0.5260 - 2s/epoch - 227ms/step\n",
      "Epoch 18/20\n",
      "8/8 - 2s - loss: 1.3236 - accuracy: 0.4744 - f1_m: 0.2849 - precision_m: 0.2682 - recall_m: 0.5729 - 2s/epoch - 225ms/step\n",
      "Epoch 19/20\n",
      "8/8 - 2s - loss: 1.1451 - accuracy: 0.5256 - f1_m: 0.2853 - precision_m: 0.1816 - recall_m: 0.7000 - 2s/epoch - 225ms/step\n",
      "Epoch 20/20\n",
      "8/8 - 2s - loss: 1.1289 - accuracy: 0.4359 - f1_m: 0.3868 - precision_m: 0.3036 - recall_m: 0.7607 - 2s/epoch - 226ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fb80825cd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not is_gpu_available():\n",
    "    print(\"No GPU found. Using CPU\")\n",
    "\n",
    "model.fit([np.array(train_input_ids), np.array(train_attention_masks)],\n",
    "          np.array(y_train_task1),batch_size=10, epochs=20, verbose=2,\n",
    "          callbacks=[WandbCallback()]\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 2s - loss: 0.8911 - accuracy: 0.3500 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 2s/epoch - 181ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13192... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▃▅▄▃▆▆▅▆▅▅▆█▅▁▇▂█▄▆▃</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>f1_m</td><td>▄▅█▇▄▄▇▇▄▄▅▄▅▅▆▁▄▄▄▆</td></tr><tr><td>loss</td><td>█▃▂▇▇▄▁▂▂▄▂▁▂▆█▂▁▃▂▂</td></tr><tr><td>precision_m</td><td>▄▆█▆▄▄▆▇▄▃▄▆▆▄▅▁▅▅▄▆</td></tr><tr><td>recall_m</td><td>▅▅▇▇▃▅▆▇▄▆▆▂▅▇█▁▄▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.4359</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>f1_m</td><td>0.38677</td></tr><tr><td>loss</td><td>1.12885</td></tr><tr><td>precision_m</td><td>0.30365</td></tr><tr><td>recall_m</td><td>0.76071</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">lively-darkness-14</strong>: <a href=\"https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/9wv4zz3b\" target=\"_blank\">https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/9wv4zz3b</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20211229_190359-9wv4zz3b\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.evaluate([np.array(test_input_ids), np.array(\n",
    "    test_attention_masks)], np.array(y_test_task1), batch_size=2, verbose=2)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Albert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/51supqb5\" target=\"_blank\">decent-silence-15</a></strong> to <a href=\"https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TFAlbertForSequenceClassification-albert-base-v2 with 2 labels\n",
      "Tokenizing data with AlbertTokenizerFast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 2982.03it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 2849.20it/s]\n",
      "All model checkpoint layers were used when initializing TFAlbertForSequenceClassification.\n",
      "\n",
      "Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_albert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " albert (TFAlbertMainLayer)  multiple                  11683584  \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,685,122\n",
      "Trainable params: 11,685,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "8/8 - 12s - loss: 2.2776 - accuracy: 0.5000 - f1_m: 0.3200 - precision_m: 0.2094 - recall_m: 0.7500 - 12s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "8/8 - 1s - loss: 1.6479 - accuracy: 0.5000 - f1_m: 0.4106 - precision_m: 0.3089 - recall_m: 0.7250 - 1s/epoch - 180ms/step\n",
      "Epoch 3/20\n",
      "8/8 - 1s - loss: 1.3361 - accuracy: 0.3974 - f1_m: 0.5502 - precision_m: 0.4170 - recall_m: 0.8333 - 1s/epoch - 169ms/step\n",
      "Epoch 4/20\n",
      "8/8 - 1s - loss: 1.6415 - accuracy: 0.5000 - f1_m: 0.4372 - precision_m: 0.3053 - recall_m: 0.9062 - 1s/epoch - 173ms/step\n",
      "Epoch 5/20\n",
      "8/8 - 1s - loss: 1.0927 - accuracy: 0.5385 - f1_m: 0.2477 - precision_m: 0.1987 - recall_m: 0.3542 - 1s/epoch - 169ms/step\n",
      "Epoch 6/20\n",
      "8/8 - 1s - loss: 1.0411 - accuracy: 0.4872 - f1_m: 0.3428 - precision_m: 0.3299 - recall_m: 0.4917 - 1s/epoch - 167ms/step\n",
      "Epoch 7/20\n",
      "8/8 - 1s - loss: 1.0473 - accuracy: 0.5513 - f1_m: 0.3304 - precision_m: 0.4021 - recall_m: 0.4116 - 1s/epoch - 171ms/step\n",
      "Epoch 8/20\n",
      "8/8 - 1s - loss: 1.0495 - accuracy: 0.5385 - f1_m: 0.3233 - precision_m: 0.2410 - recall_m: 0.6095 - 1s/epoch - 169ms/step\n",
      "Epoch 9/20\n",
      "8/8 - 1s - loss: 1.2076 - accuracy: 0.5385 - f1_m: 0.2574 - precision_m: 0.2603 - recall_m: 0.6750 - 1s/epoch - 168ms/step\n",
      "Epoch 10/20\n",
      "8/8 - 1s - loss: 1.4027 - accuracy: 0.4231 - f1_m: 0.4200 - precision_m: 0.2875 - recall_m: 0.9792 - 1s/epoch - 169ms/step\n",
      "Epoch 11/20\n",
      "8/8 - 1s - loss: 1.5849 - accuracy: 0.5000 - f1_m: 0.4514 - precision_m: 0.3070 - recall_m: 1.0250 - 1s/epoch - 169ms/step\n",
      "Epoch 12/20\n",
      "8/8 - 1s - loss: 1.1754 - accuracy: 0.4615 - f1_m: 0.2275 - precision_m: 0.1500 - recall_m: 0.5625 - 1s/epoch - 169ms/step\n",
      "Epoch 13/20\n",
      "8/8 - 1s - loss: 1.0835 - accuracy: 0.5128 - f1_m: 0.4097 - precision_m: 0.3661 - recall_m: 0.8375 - 1s/epoch - 170ms/step\n",
      "Epoch 14/20\n",
      "8/8 - 1s - loss: 0.9318 - accuracy: 0.6538 - f1_m: 0.2435 - precision_m: 0.1741 - recall_m: 0.4107 - 1s/epoch - 166ms/step\n",
      "Epoch 15/20\n",
      "8/8 - 1s - loss: 1.4026 - accuracy: 0.4487 - f1_m: 0.3087 - precision_m: 0.1958 - recall_m: 0.7812 - 1s/epoch - 169ms/step\n",
      "Epoch 16/20\n",
      "8/8 - 1s - loss: 1.0703 - accuracy: 0.6410 - f1_m: 0.4133 - precision_m: 0.3653 - recall_m: 0.6548 - 1s/epoch - 167ms/step\n",
      "Epoch 17/20\n",
      "8/8 - 1s - loss: 1.4617 - accuracy: 0.4872 - f1_m: 0.2187 - precision_m: 0.1361 - recall_m: 0.7188 - 1s/epoch - 167ms/step\n",
      "Epoch 18/20\n",
      "8/8 - 1s - loss: 1.0321 - accuracy: 0.4231 - f1_m: 0.4703 - precision_m: 0.3947 - recall_m: 0.7729 - 1s/epoch - 170ms/step\n",
      "Epoch 19/20\n",
      "8/8 - 1s - loss: 1.1034 - accuracy: 0.5128 - f1_m: 0.2507 - precision_m: 0.2736 - recall_m: 0.5208 - 1s/epoch - 168ms/step\n",
      "Epoch 20/20\n",
      "8/8 - 1s - loss: 1.2385 - accuracy: 0.5513 - f1_m: 0.4319 - precision_m: 0.3073 - recall_m: 0.8625 - 1s/epoch - 178ms/step\n",
      "10/10 - 2s - loss: 1.1585 - accuracy: 0.3500 - f1_m: 0.4333 - precision_m: 0.3500 - recall_m: 0.6000 - 2s/epoch - 166ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23084... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▄▄▁▄▅▃▅▅▅▂▄▃▄█▂█▃▂▄▅</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>f1_m</td><td>▃▅█▆▂▄▃▃▂▅▆▁▅▂▃▅▁▆▂▆</td></tr><tr><td>loss</td><td>█▅▃▅▂▂▂▂▂▃▄▂▂▁▃▂▄▂▂▃</td></tr><tr><td>precision_m</td><td>▃▅█▅▃▆█▄▄▅▅▁▇▂▂▇▁▇▄▅</td></tr><tr><td>recall_m</td><td>▅▅▆▇▁▂▂▄▄██▃▆▂▅▄▅▅▃▆</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.55128</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>f1_m</td><td>0.43189</td></tr><tr><td>loss</td><td>1.23849</td></tr><tr><td>precision_m</td><td>0.30729</td></tr><tr><td>recall_m</td><td>0.8625</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">decent-silence-15</strong>: <a href=\"https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/51supqb5\" target=\"_blank\">https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/51supqb5</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20211229_190859-51supqb5\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=WANDB_PROJECT_NAME, job_type=\"training\")\n",
    "model, train_input_ids, train_attention_masks, test_input_ids, test_attention_masks = get_model_and_data(TFAlbertForSequenceClassification, 2, X_train, X_test)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                                  loss=hinge,\n",
    "                                  metrics=['accuracy', f1_m,precision_m, recall_m])\n",
    "                                  \n",
    "if not is_gpu_available():\n",
    "    print(\"No GPU found. Using CPU\")\n",
    "\n",
    "model.fit([np.array(train_input_ids), np.array(train_attention_masks)],\n",
    "          np.array(y_train_task1),batch_size=10, epochs=20, verbose=2,\n",
    "          callbacks=[WandbCallback()]\n",
    "          )\n",
    "\n",
    "model.evaluate([np.array(test_input_ids), np.array(test_attention_masks)], np.array(y_test_task1), batch_size=2, verbose=2)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maleksandar1932\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "d:\\envs\\nlp_project\\lib\\site-packages\\IPython\\html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/1fj2lwyo\" target=\"_blank\">drawn-universe-17</a></strong> to <a href=\"https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TFRobertaForSequenceClassification-roberta-base with 2 labels\n",
      "Tokenizing data with RobertaTokenizerFast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 3397.43it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 3307.94it/s]\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLayer  multiple                 124055040 \n",
      " )                                                               \n",
      "                                                                 \n",
      " classifier (TFRobertaClassi  multiple                 592130    \n",
      " ficationHead)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124,647,170\n",
      "Trainable params: 124,647,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\ALEKSA~1\\AppData\\Local\\Temp/ipykernel_5080/1127252575.py:9: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Epoch 1/20\n",
      "8/8 - 17s - loss: 1.6940 - accuracy: 0.4744 - f1_m: 0.2184 - precision_m: 0.2750 - recall_m: 0.4271 - 17s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "8/8 - 2s - loss: 1.1995 - accuracy: 0.5000 - f1_m: 0.3801 - precision_m: 0.2679 - recall_m: 0.7917 - 2s/epoch - 262ms/step\n",
      "Epoch 3/20\n",
      "8/8 - 2s - loss: 1.2868 - accuracy: 0.4744 - f1_m: 0.3402 - precision_m: 0.2500 - recall_m: 0.6250 - 2s/epoch - 263ms/step\n",
      "Epoch 4/20\n",
      "8/8 - 2s - loss: 1.5382 - accuracy: 0.4359 - f1_m: 0.3214 - precision_m: 0.2562 - recall_m: 0.6845 - 2s/epoch - 263ms/step\n",
      "Epoch 5/20\n",
      "8/8 - 2s - loss: 1.2962 - accuracy: 0.4231 - f1_m: 0.3229 - precision_m: 0.2125 - recall_m: 0.6750 - 2s/epoch - 270ms/step\n",
      "Epoch 6/20\n",
      "8/8 - 2s - loss: 1.2410 - accuracy: 0.5385 - f1_m: 0.2305 - precision_m: 0.2593 - recall_m: 0.3958 - 2s/epoch - 264ms/step\n",
      "Epoch 7/20\n",
      "8/8 - 2s - loss: 1.3175 - accuracy: 0.4872 - f1_m: 0.4144 - precision_m: 0.2841 - recall_m: 0.8687 - 2s/epoch - 250ms/step\n",
      "Epoch 8/20\n",
      "8/8 - 2s - loss: 1.2934 - accuracy: 0.4872 - f1_m: 0.3298 - precision_m: 0.2207 - recall_m: 0.7250 - 2s/epoch - 249ms/step\n",
      "Epoch 9/20\n",
      "8/8 - 2s - loss: 1.2500 - accuracy: 0.4872 - f1_m: 0.3305 - precision_m: 0.2277 - recall_m: 0.6938 - 2s/epoch - 250ms/step\n",
      "Epoch 10/20\n",
      "8/8 - 2s - loss: 1.5270 - accuracy: 0.4615 - f1_m: 0.3378 - precision_m: 0.2664 - recall_m: 0.8036 - 2s/epoch - 249ms/step\n",
      "Epoch 11/20\n",
      "8/8 - 2s - loss: 1.0521 - accuracy: 0.5641 - f1_m: 0.0417 - precision_m: 0.1250 - recall_m: 0.0250 - 2s/epoch - 261ms/step\n",
      "Epoch 12/20\n",
      "8/8 - 2s - loss: 0.9979 - accuracy: 0.5513 - f1_m: 0.5208 - precision_m: 0.3971 - recall_m: 0.8917 - 2s/epoch - 249ms/step\n",
      "Epoch 13/20\n",
      "8/8 - 2s - loss: 1.6072 - accuracy: 0.4744 - f1_m: 0.3445 - precision_m: 0.2375 - recall_m: 0.6500 - 2s/epoch - 248ms/step\n",
      "Epoch 14/20\n",
      "8/8 - 2s - loss: 1.2588 - accuracy: 0.5128 - f1_m: 0.4461 - precision_m: 0.3653 - recall_m: 0.6750 - 2s/epoch - 252ms/step\n",
      "Epoch 15/20\n",
      "8/8 - 2s - loss: 1.0388 - accuracy: 0.4872 - f1_m: 0.3302 - precision_m: 0.3205 - recall_m: 0.6771 - 2s/epoch - 249ms/step\n",
      "Epoch 16/20\n",
      "8/8 - 2s - loss: 1.0167 - accuracy: 0.4103 - f1_m: 0.4529 - precision_m: 0.3740 - recall_m: 0.7774 - 2s/epoch - 253ms/step\n",
      "Epoch 17/20\n",
      "8/8 - 2s - loss: 1.1596 - accuracy: 0.4872 - f1_m: 0.4229 - precision_m: 0.3045 - recall_m: 0.7708 - 2s/epoch - 255ms/step\n",
      "Epoch 18/20\n",
      "8/8 - 2s - loss: 1.4247 - accuracy: 0.5000 - f1_m: 0.4290 - precision_m: 0.2988 - recall_m: 0.8438 - 2s/epoch - 251ms/step\n",
      "Epoch 19/20\n",
      "8/8 - 2s - loss: 1.1643 - accuracy: 0.5256 - f1_m: 0.2401 - precision_m: 0.1917 - recall_m: 0.3458 - 2s/epoch - 250ms/step\n",
      "Epoch 20/20\n",
      "8/8 - 2s - loss: 1.4429 - accuracy: 0.4744 - f1_m: 0.2715 - precision_m: 0.1681 - recall_m: 0.7500 - 2s/epoch - 249ms/step\n",
      "10/10 - 2s - loss: 1.2314 - accuracy: 0.3500 - f1_m: 0.5333 - precision_m: 0.3500 - recall_m: 1.2000 - 2s/epoch - 171ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6676... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▄▅▄▂▂▇▄▄▄▃█▇▄▆▄▁▄▅▆▄</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>f1_m</td><td>▄▆▅▅▅▄▆▅▅▅▁█▅▇▅▇▇▇▄▄</td></tr><tr><td>loss</td><td>█▃▄▆▄▃▄▄▄▆▂▁▇▄▁▁▃▅▃▅</td></tr><tr><td>precision_m</td><td>▅▅▄▄▃▄▅▃▄▅▁█▄▇▆▇▆▅▃▂</td></tr><tr><td>recall_m</td><td>▄▇▆▆▆▄█▇▆▇▁█▆▆▆▇▇█▄▇</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.47436</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>f1_m</td><td>0.27151</td></tr><tr><td>loss</td><td>1.4429</td></tr><tr><td>precision_m</td><td>0.16806</td></tr><tr><td>recall_m</td><td>0.75</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">drawn-universe-17</strong>: <a href=\"https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/1fj2lwyo\" target=\"_blank\">https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/1fj2lwyo</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20211229_191736-1fj2lwyo\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=WANDB_PROJECT_NAME, job_type=\"training\")\n",
    "model, train_input_ids, train_attention_masks, test_input_ids, test_attention_masks = get_model_and_data(TFRobertaForSequenceClassification, 2, X_train, X_test)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                                  loss=hinge,\n",
    "                                  metrics=['accuracy', f1_m,precision_m, recall_m])\n",
    "                                  \n",
    "if not is_gpu_available():\n",
    "    print(\"No GPU found. Using CPU\")\n",
    "\n",
    "model.fit([np.array(train_input_ids), np.array(train_attention_masks)],\n",
    "          np.array(y_train_task1),batch_size=10, epochs=20, verbose=2,\n",
    "          callbacks=[WandbCallback()]\n",
    "          )\n",
    "\n",
    "model.evaluate([np.array(test_input_ids), np.array(test_attention_masks)], np.array(y_test_task1), batch_size=2, verbose=2)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/2kxn85j4\" target=\"_blank\">dainty-snow-18</a></strong> to <a href=\"https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TFDistilBertForSequenceClassification-distilbert-base-cased with 2 labels\n",
      "Tokenizing data with DistilBertTokenizerFast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 3781.27it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 3971.50it/s]\n",
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['dropout_57', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 65190912  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,783,042\n",
      "Trainable params: 65,783,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "8/8 - 9s - loss: 4.4850 - accuracy: 0.5256 - f1_m: 0.3460 - precision_m: 0.3170 - recall_m: 0.6875 - 9s/epoch - 1s/step\n",
      "Epoch 2/20\n",
      "8/8 - 1s - loss: 2.3631 - accuracy: 0.5513 - f1_m: 0.2130 - precision_m: 0.1375 - recall_m: 0.5000 - 1s/epoch - 140ms/step\n",
      "Epoch 3/20\n",
      "8/8 - 1s - loss: 1.0189 - accuracy: 0.5256 - f1_m: 0.0357 - precision_m: 0.1250 - recall_m: 0.0208 - 1s/epoch - 142ms/step\n",
      "Epoch 4/20\n",
      "8/8 - 1s - loss: 0.9713 - accuracy: 0.4231 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 139ms/step\n",
      "Epoch 5/20\n",
      "8/8 - 1s - loss: 1.0304 - accuracy: 0.5000 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 138ms/step\n",
      "Epoch 6/20\n",
      "8/8 - 1s - loss: 1.0076 - accuracy: 0.4744 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 137ms/step\n",
      "Epoch 7/20\n",
      "8/8 - 1s - loss: 1.0034 - accuracy: 0.5000 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 138ms/step\n",
      "Epoch 8/20\n",
      "8/8 - 1s - loss: 0.9503 - accuracy: 0.4872 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 133ms/step\n",
      "Epoch 9/20\n",
      "8/8 - 1s - loss: 1.0435 - accuracy: 0.5000 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 139ms/step\n",
      "Epoch 10/20\n",
      "8/8 - 1s - loss: 0.9971 - accuracy: 0.5128 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 136ms/step\n",
      "Epoch 11/20\n",
      "8/8 - 1s - loss: 1.0012 - accuracy: 0.5128 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 134ms/step\n",
      "Epoch 12/20\n",
      "8/8 - 1s - loss: 1.0201 - accuracy: 0.4359 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 132ms/step\n",
      "Epoch 13/20\n",
      "8/8 - 1s - loss: 0.9779 - accuracy: 0.5641 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 133ms/step\n",
      "Epoch 14/20\n",
      "8/8 - 1s - loss: 1.0056 - accuracy: 0.5000 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 132ms/step\n",
      "Epoch 15/20\n",
      "8/8 - 1s - loss: 0.9914 - accuracy: 0.5513 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 132ms/step\n",
      "Epoch 16/20\n",
      "8/8 - 1s - loss: 0.9804 - accuracy: 0.5256 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 132ms/step\n",
      "Epoch 17/20\n",
      "8/8 - 1s - loss: 1.0248 - accuracy: 0.5256 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 134ms/step\n",
      "Epoch 18/20\n",
      "8/8 - 1s - loss: 0.9964 - accuracy: 0.5769 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 132ms/step\n",
      "Epoch 19/20\n",
      "8/8 - 1s - loss: 0.9958 - accuracy: 0.5000 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - 1s/epoch - 134ms/step\n",
      "Epoch 20/20\n",
      "8/8 - 1s - loss: 0.9628 - accuracy: 0.6026 - f1_m: 0.1176 - precision_m: 0.0833 - recall_m: 0.2000 - 1s/epoch - 139ms/step\n",
      "10/10 - 1s - loss: 1.4065 - accuracy: 0.3500 - f1_m: 0.5333 - precision_m: 0.3500 - recall_m: 1.2000 - 994ms/epoch - 99ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11864... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▅▆▅▁▄▃▄▄▄▅▅▁▇▄▆▅▅▇▄█</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>f1_m</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃</td></tr><tr><td>loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision_m</td><td>█▄▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃</td></tr><tr><td>recall_m</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.60256</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>f1_m</td><td>0.11765</td></tr><tr><td>loss</td><td>0.96282</td></tr><tr><td>precision_m</td><td>0.08333</td></tr><tr><td>recall_m</td><td>0.2</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dainty-snow-18</strong>: <a href=\"https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/2kxn85j4\" target=\"_blank\">https://wandb.ai/aleksandar1932/%5BNLP%5D%20lab-04%20%7C%20misogyny%20classification/runs/2kxn85j4</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20211229_191928-2kxn85j4\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=WANDB_PROJECT_NAME, job_type=\"training\")\n",
    "model, train_input_ids, train_attention_masks, test_input_ids, test_attention_masks = get_model_and_data(TFDistilBertForSequenceClassification, 2, X_train, X_test)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                                  loss=hinge,\n",
    "                                  metrics=['accuracy', f1_m,precision_m, recall_m])\n",
    "                                  \n",
    "if not is_gpu_available():\n",
    "    print(\"No GPU found. Using CPU\")\n",
    "\n",
    "model.fit([np.array(train_input_ids), np.array(train_attention_masks)],\n",
    "          np.array(y_train_task1),batch_size=10, epochs=20, verbose=2,\n",
    "          callbacks=[WandbCallback()]\n",
    "          )\n",
    "\n",
    "model.evaluate([np.array(test_input_ids), np.array(test_attention_masks)], np.array(y_test_task1), batch_size=2, verbose=2)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500\"\n",
       "            src=\"https://wandb.ai/aleksandar1932/[NLP]%20lab-04%20%7C%20misogyny%20classification/reports/Task-1--VmlldzoxMzg3OTU0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1fde49442e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://wandb.ai/aleksandar1932/[NLP]%20lab-04%20%7C%20misogyny%20classification/reports/Task-1--VmlldzoxMzg3OTU0', width=\"100%\", height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=WANDB_PROJECT_NAME, job_type=\"training\")\n",
    "model, train_input_ids, train_attention_masks, test_input_ids, test_attention_masks = get_model_and_data(\n",
    "    TFBertForSequenceClassification, num_classes, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.01), loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\", f1_m, precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([np.array(train_input_ids), np.array(train_attention_masks)],\n",
    "          np.array(y_train_task2), batch_size=70, epochs=1, verbose=2,\n",
    "          callbacks=[WandbCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([np.array(test_input_ids), np.array(\n",
    "    test_attention_masks)], np.array(y_test_task2), batch_size=70, verbose=2)\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Albert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02d0b1b3f612c4c5e51656c8d0ea12cc8bdc13c9ac193c394dc1e17c8d0fd734"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('nlp-2021-n': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
