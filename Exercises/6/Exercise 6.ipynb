{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_embeddings(file_name, vocabulary):\n",
    "    \"\"\"\n",
    "    Loads word embeddings from the file with the given name.\n",
    "    :param file_name: name of the file containing word embeddings\n",
    "    :type file_name: str\n",
    "    :param vocabulary: captions vocabulary\n",
    "    :type vocabulary: numpy.array\n",
    "    :return: word embeddings\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    embeddings = dict()\n",
    "    with open(file_name, 'r', encoding='utf-8') as doc:\n",
    "        line = doc.readline()\n",
    "        while line != '':\n",
    "            line = line.rstrip('\\n').lower()\n",
    "            parts = line.split(' ')\n",
    "            vals = np.array(parts[1:], dtype=np.float)\n",
    "            if parts[0] in vocabulary:\n",
    "                embeddings[parts[0]] = vals\n",
    "            line = doc.readline()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_embedding_weights(vocabulary, embedding_size, embedding_type, path='.'):\n",
    "    print(\"local\")\n",
    "    \"\"\"\n",
    "    Creates and loads embedding weights.\n",
    "    :param vocabulary: vocabulary\n",
    "    :type vocabulary: numpy.array\n",
    "    :param embedding_size: embedding size\n",
    "    :type embedding_size: int\n",
    "    :param embedding_type: type of the pre-trained embeddings\n",
    "    :type embedding_type: string\n",
    "    :return: embedding weights\n",
    "    :rtype: numpy.array\n",
    "    \"\"\"\n",
    "    if os.path.exists(f'{path}/embedding_matrix_{embedding_type}_{embedding_size}.pkl'):\n",
    "        with open(f'{path}/embedding_matrix_{embedding_type}_{embedding_size}.pkl', 'rb') as f:\n",
    "            embedding_matrix = pickle.load(f)\n",
    "    else:\n",
    "        print('Creating embedding weights...')\n",
    "        if embedding_type == 'glove':\n",
    "            embeddings = load_embeddings(f'{path}/glove.6B.{embedding_size}d.txt', vocabulary)\n",
    "        else:\n",
    "          embeddings = load_embeddings(f'{path}/word2vecSG.iSarcasamEval.{embedding_size}d.txt', vocabulary)\n",
    "        embedding_matrix = np.zeros((len(vocabulary), embedding_size))\n",
    "        for i in range(len(vocabulary)):\n",
    "            if vocabulary[i] in embeddings.keys():\n",
    "                embedding_matrix[i] = embeddings[vocabulary[i]]\n",
    "            else:\n",
    "                embedding_matrix[i] = np.random.standard_normal(embedding_size)\n",
    "        with open(f'{path}/embedding_matrix_{embedding_type}_{embedding_size}.pkl', 'wb') as f:\n",
    "            pickle.dump(embedding_matrix, f)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 12:21:17.200772: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-05 12:21:17.200811: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from scripts.word_embeddings import load_embedding_weights\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    return pd.read_csv('data/train.En.csv', usecols=['tweet', 'rephrase']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    data['tweet_tokens'] = data['tweet'].apply(lambda x: word_tokenize(x.lower()))\n",
    "    data['rephrase_tokens'] = data['rephrase'].apply(lambda x: word_tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_start_end(data):\n",
    "    data['tweet_tokens'] = data['tweet_tokens'].apply(lambda x: np.concatenate((['<START>'], x, ['</END>'])))\n",
    "    data['rephrase_tokens'] = data['rephrase_tokens'].apply(lambda x: np.concatenate((['<START>'], x, ['</END>'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(sentence_tokens):\n",
    "    vocab = set()\n",
    "    for tokens in sentence_tokens:\n",
    "        vocab.update(tokens)\n",
    "    \n",
    "    vocab = list(vocab)\n",
    "    word_to_id = {word: index for word, index in zip(vocab, range(len(vocab)))}\n",
    "    id_to_word = {index: word for word, index in zip(vocab, range(len(vocab)))}\n",
    "    return vocab, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(sentences, rephrases):\n",
    "    input_sentences, input_rephrases, next_words = [], [], []\n",
    "\n",
    "    for sentence, rephrase in zip(sentences, rephrases):\n",
    "        for i in range(1, len(rephrase)):\n",
    "            input_sentences.append(sentence)\n",
    "            input_rephrases.append(rephrase[:i])\n",
    "            next_words.append(rephrase[i])\n",
    "\n",
    "    return input_sentences, input_rephrases, next_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(padding_size, vocabulary_size, embedding_size, embeddings=None, name=\"\"):\n",
    "    # encoder\n",
    "    encoder_inputs = Input(shape=(padding_size,), name='encoder_inputs')\n",
    "    encoder_embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_size,\n",
    "                                #   weights=[embeddings], \n",
    "                                  trainable=False)(encoder_inputs)\n",
    "\n",
    "    encoder = LSTM(128, return_state=True, name='encoder')\n",
    "    encoder(encoder_embedding)\n",
    "\n",
    "    _, state_h, state_c = encoder(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # decoder\n",
    "    decoder_inputs = Input(shape=(padding_size,), name='decoder_inputs')\n",
    "    decoder_embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_size,\n",
    "                                #   weights=[embeddings], \n",
    "                                  trainable=False)(decoder_inputs)\n",
    "\n",
    "    decoder = LSTM(128, return_state=True, name='decoder')\n",
    "    decoder_outputs, _, _ =decoder(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "    decoder_outputs = Dense(vocabulary_size, activation='softmax', name='decoder_dense')(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "    model._name = name\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model, input_sent, word_to_id, padding_size):\n",
    "    generated_sentence = []\n",
    "    generated_sentence.append(word_to_id['<START>'])\n",
    "\n",
    "    for i in range(padding_size):\n",
    "        output_sent = pad_sequences([generated_sentence], padding_size)\n",
    "        predictions = model.predict([np.expand_dims(input_sent, axis=0), output_sent])\n",
    "        next_word = np.argmax(predictions)\n",
    "        generated_sentence.append(next_word)\n",
    "\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentences, id_to_word):\n",
    "    output_sentences = []\n",
    "    for sent in sentences:\n",
    "        output_sentences.append(' '.join([id_to_word[i] for i in sent]))\n",
    "\n",
    "    return output_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>rephrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>College is really difficult, expensive, tiring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>I do not like when professors don’t write out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  The only thing I got from college is a caffein...   \n",
       "1  I love it when professors draw a big question ...   \n",
       "2  Remember the hundred emails from companies whe...   \n",
       "3  Today my pop-pop told me I was not “forced” to...   \n",
       "4  @VolphanCarol @littlewhitty @mysticalmanatee I...   \n",
       "\n",
       "                                            rephrase  \n",
       "0  College is really difficult, expensive, tiring...  \n",
       "1  I do not like when professors don’t write out ...  \n",
       "2  I, at the bare minimum, wish companies actuall...  \n",
       "3  Today my pop-pop told me I was not \"forced\" to...  \n",
       "4  I would say Ted Cruz is an asshole and doesn’t...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>rephrase_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>College is really difficult, expensive, tiring...</td>\n",
       "      <td>[the, only, thing, i, got, from, college, is, ...</td>\n",
       "      <td>[college, is, really, difficult, ,, expensive,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>I do not like when professors don’t write out ...</td>\n",
       "      <td>[i, love, it, when, professors, draw, a, big, ...</td>\n",
       "      <td>[i, do, not, like, when, professors, don, ’, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
       "      <td>[remember, the, hundred, emails, from, compani...</td>\n",
       "      <td>[i, ,, at, the, bare, minimum, ,, wish, compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
       "      <td>[today, my, pop-pop, told, me, i, was, not, “,...</td>\n",
       "      <td>[today, my, pop-pop, told, me, i, was, not, ``...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
       "      <td>[@, volphancarol, @, littlewhitty, @, mystical...</td>\n",
       "      <td>[i, would, say, ted, cruz, is, an, asshole, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  The only thing I got from college is a caffein...   \n",
       "1  I love it when professors draw a big question ...   \n",
       "2  Remember the hundred emails from companies whe...   \n",
       "3  Today my pop-pop told me I was not “forced” to...   \n",
       "4  @VolphanCarol @littlewhitty @mysticalmanatee I...   \n",
       "\n",
       "                                            rephrase  \\\n",
       "0  College is really difficult, expensive, tiring...   \n",
       "1  I do not like when professors don’t write out ...   \n",
       "2  I, at the bare minimum, wish companies actuall...   \n",
       "3  Today my pop-pop told me I was not \"forced\" to...   \n",
       "4  I would say Ted Cruz is an asshole and doesn’t...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [the, only, thing, i, got, from, college, is, ...   \n",
       "1  [i, love, it, when, professors, draw, a, big, ...   \n",
       "2  [remember, the, hundred, emails, from, compani...   \n",
       "3  [today, my, pop-pop, told, me, i, was, not, “,...   \n",
       "4  [@, volphancarol, @, littlewhitty, @, mystical...   \n",
       "\n",
       "                                     rephrase_tokens  \n",
       "0  [college, is, really, difficult, ,, expensive,...  \n",
       "1  [i, do, not, like, when, professors, don, ’, t...  \n",
       "2  [i, ,, at, the, bare, minimum, ,, wish, compan...  \n",
       "3  [today, my, pop-pop, told, me, i, was, not, ``...  \n",
       "4  [i, would, say, ted, cruz, is, an, asshole, an...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>rephrase_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>College is really difficult, expensive, tiring...</td>\n",
       "      <td>[&lt;START&gt;, the, only, thing, i, got, from, coll...</td>\n",
       "      <td>[&lt;START&gt;, college, is, really, difficult, ,, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>I do not like when professors don’t write out ...</td>\n",
       "      <td>[&lt;START&gt;, i, love, it, when, professors, draw,...</td>\n",
       "      <td>[&lt;START&gt;, i, do, not, like, when, professors, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
       "      <td>[&lt;START&gt;, remember, the, hundred, emails, from...</td>\n",
       "      <td>[&lt;START&gt;, i, ,, at, the, bare, minimum, ,, wis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
       "      <td>[&lt;START&gt;, today, my, pop-pop, told, me, i, was...</td>\n",
       "      <td>[&lt;START&gt;, today, my, pop-pop, told, me, i, was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
       "      <td>[&lt;START&gt;, @, volphancarol, @, littlewhitty, @,...</td>\n",
       "      <td>[&lt;START&gt;, i, would, say, ted, cruz, is, an, as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  The only thing I got from college is a caffein...   \n",
       "1  I love it when professors draw a big question ...   \n",
       "2  Remember the hundred emails from companies whe...   \n",
       "3  Today my pop-pop told me I was not “forced” to...   \n",
       "4  @VolphanCarol @littlewhitty @mysticalmanatee I...   \n",
       "\n",
       "                                            rephrase  \\\n",
       "0  College is really difficult, expensive, tiring...   \n",
       "1  I do not like when professors don’t write out ...   \n",
       "2  I, at the bare minimum, wish companies actuall...   \n",
       "3  Today my pop-pop told me I was not \"forced\" to...   \n",
       "4  I would say Ted Cruz is an asshole and doesn’t...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [<START>, the, only, thing, i, got, from, coll...   \n",
       "1  [<START>, i, love, it, when, professors, draw,...   \n",
       "2  [<START>, remember, the, hundred, emails, from...   \n",
       "3  [<START>, today, my, pop-pop, told, me, i, was...   \n",
       "4  [<START>, @, volphancarol, @, littlewhitty, @,...   \n",
       "\n",
       "                                     rephrase_tokens  \n",
       "0  [<START>, college, is, really, difficult, ,, e...  \n",
       "1  [<START>, i, do, not, like, when, professors, ...  \n",
       "2  [<START>, i, ,, at, the, bare, minimum, ,, wis...  \n",
       "3  [<START>, today, my, pop-pop, told, me, i, was...  \n",
       "4  [<START>, i, would, say, ted, cruz, is, an, as...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "append_start_end(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['tweet_tokens'].values\n",
    "rephrases = df['rephrase_tokens'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, word_to_id, id_to_word = create_vocabulary(np.concatenate((sentences, rephrases)))\n",
    "\n",
    "df['tweet_indices'] = df['tweet_tokens'].apply(lambda x: np.array([word_to_id[i] for i in x]))\n",
    "sentence_indices = df['tweet_indices'].values\n",
    "\n",
    "df['rephrase_indices'] = df['rephrase_tokens'].apply(lambda x: np.array([word_to_id[i] for i in x]))\n",
    "rephrase_indices = df['rephrase_indices'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embedding_weights(vocabulary, 50, 'glove', \"/mnt/d/Downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_rephrases, \\\n",
    "    test_sentences, test_rephrases = train_test_split(sentence_indices, rephrase_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "test_sentences, test_rephrases = test_sentences[:5], test_rephrases[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences, input_rephrases, next_words = create_train_data(train_sentences, train_rephrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sentences = pad_sequences(input_sentences, maxlen=10)\n",
    "padded_rephrases = pad_sequences(input_rephrases, maxlen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(list(word_to_id.values()))\n",
    "next_words = label_binarizer.transform(next_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksandar/envs/nlp-2021-n/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1957/2477508616.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentece\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpadded_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutput_reprhases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentece\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1957/914078849.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(model, input_sent, word_to_id, padding_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutput_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgenerated_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgenerated_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/nlp-2021-n/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/nlp-2021-n/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1756\u001b[0m               stacklevel=2)\n\u001b[1;32m   1757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1758\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1759\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1760\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/nlp-2021-n/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/nlp-2021-n/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1154\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/nlp-2021-n/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/nlp-2021-n/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    368\u001b[0m       options.experimental_external_state_policy = (\n\u001b[1;32m    369\u001b[0m           tf.data.experimental.ExternalStatePolicy.IGNORE)\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/nlp-2021-n/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwith_options\u001b[0;34m(self, options, name)\u001b[0m\n\u001b[1;32m   2684\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0man\u001b[0m \u001b[0moption\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mset\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0monce\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2685\u001b[0m     \"\"\"\n\u001b[0;32m-> 2686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_OptionsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2688\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/nlp-2021-n/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, options, name)\u001b[0m\n\u001b[1;32m   5826\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5827\u001b[0m     \u001b[0moptions_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_options_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5828\u001b[0;31m     \u001b[0moptions_pb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5829\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5830\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/nlp-2021-n/lib/python3.8/site-packages/tensorflow/python/data/ops/options.py\u001b[0m in \u001b[0;36m_to_proto\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m       \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautotune_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautotune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_external_state_policy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = create_model(10, len(vocabulary), 50, embeddings, name='lstm_seq2seq')\n",
    "model.load_weights('../5/EncoderDecoder.h5')\n",
    "\n",
    "output_reprhases = []\n",
    "\n",
    "for sentece in padded_sentences:\n",
    "    output_reprhases.append(decode(model, sentece, word_to_id, 10))\n",
    "\n",
    "input_sentences = convert(test_sentences, id_to_word)\n",
    "gt_reprahes = convert(test_rephrases, id_to_word)\n",
    "pred_reprhases = convert(output_reprhases, id_to_word)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02d0b1b3f612c4c5e51656c8d0ea12cc8bdc13c9ac193c394dc1e17c8d0fd734"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('nlp-2021-n': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
